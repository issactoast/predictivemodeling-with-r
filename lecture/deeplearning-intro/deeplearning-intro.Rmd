---
title: "Introduction to deeplearning"
subtitle: "Neural Net training with Torch"
author: "Issac"
institute: "성균관대학교"
date: "2021년 5월 27일"
output:
  xaringan::moon_reader:
    css: 
      - default
      - css/statisticsplaybook.css
      - css/statisticsplaybook-fonts.css
    lib_dir: libs
    seal: false
    nature:
      highlightStyle: googlecode
      highlightLines: true
      highlightLanguage: ["r"]
      countIncrementalSlides: false
      ratio: "16:9"
    includes:
      after_body: [css/insert-logo.html]
---

```{r setup, include = FALSE}
library(knitr)
library(tidyverse)
library(torch)
xaringanExtra::use_xaringan_extra(c("tile_view", "panelset", "editable"))
# set default options
opts_chunk$set(dpi = 300)
```

class: title-slide, left, bottom

# `r rmarkdown::metadata$title`
----
## **`r rmarkdown::metadata$subtitle`**
### `r rmarkdown::metadata$author`
### `r rmarkdown::metadata$date`

---
class: inverse, middle, center

# 지난시간 복습

Neural Net 생성 코드

---
# 신경망 생성 절차

.left-column[
## nn_module
]
.right-column[

기본적으로 세가지 요소가 필요

* classname

* initialize

  신경망 객체가 만들어질때 처음으로 실행되는 함수
  신경망에 들어가는 요소들 (layer, activation function)을 정의하는 단계

* forward

  실제 신경망 구조를 정의
  데이터가 들어와서 나가는 흐름을 정의하는 단계
]

---
# 신경망 생성 예제

.left-column[
## 1단 신경망
]
.right-column[

```{r}
TwoLayerNet <- nn_module(
    classname = "TowLayerNet",
    initialize = function(data_in, hidden, data_out){
        self$hidden1 <- nn_linear(data_in, hidden)
        self$output_layer <- nn_linear(hidden, data_out)
        self$tanh <- nn_tanh()
    },
    forward = function(X) {
        x <- self$tanh(self$hidden1(X))
        y_hat <- self$output_layer(x)
        return(y_hat)
    }
)
```

]

---
class: inverse, middle, center

# 신경망 학습 구조

Neural Net training with `torch`

---
# 신경망 학습에 필요한 것들

.left-column[

]
.right-column[
* 신경망

```{r }
my_net <- TwoLayerNet(8, 10, 3)
```

* 손실함수

    알맞은 손실함수를 `torch` 라이브러리에서 선택

```{r eval=FALSE}
mse_loss <- nn_mse_loss(reduction = "mean")
```


* 경사하강법 구현

```{r eval=FALSE}
optimizer <- optim_sgd(my_net$parameters, lr = 1e-5)
```


]

---
# 신경망 학습 구조

.panelset[
.panel[.panel-name[손실함수계산]

.left-column[

]
.right-column[

```{r eval=FALSE}
store_loss <- rep(0, 50000)
for (epoch in 1:50000){
    optimizer$zero_grad()
*   output <- my_net(x_tensor)
*   loss <- mse_loss(output, y_tensor)
    loss$backward()
    optimizer$step()
    store_loss[epoch] <- as.numeric(loss$item())
  
    if (epoch %% 5000 == 0){
        cat(sprintf("Loss at epoch %d: %.2f\n", epoch, store_loss[epoch]))
    }
}
```

]

]
.panel[.panel-name[경사하강법]

.left-column[

]
.right-column[

```{r eval=FALSE}
store_loss <- rep(0, 50000)
for (epoch in 1:50000){
*   optimizer$zero_grad()
    output <- my_net(x_tensor)
    loss <- mse_loss(output, y_tensor)
*   loss$backward()
*   optimizer$step()
    store_loss[epoch] <- as.numeric(loss$item())
  
    if (epoch %% 5000 == 0){
        cat(sprintf("Loss at epoch %d: %.2f\n", epoch, store_loss[epoch]))
    }
}
```

]

]

]

---
class: inverse, middle, center

# 데이터 준비하기

Dataset & Dataloader in `torch`

---
# `torch` 데이터셋

* 잊지말아야 할 사실! `torch`와 `R`은 엄연히 다름.

.left-column[
## Dataset 클래스 생성자
]
.right-column[

신경망 생성과 거의 같은 구조로 데이터셋 객체를 만들어 줌.

* name

* initialize

* .getitem

* .length

]

---
# `torch` 데이터셋 예제

.panelset[
.panel[.panel-name[펭귄데이터]
.left-column[

]
.right-column[

```{r}
library(palmerpenguins)
penguins %>% head(5)
```

]

]
.panel[.panel-name[전처리]

.left-column[

]
.right-column[
```{r message=FALSE, warning=FALSE}
library(recipes)

penguin_data <- penguins %>% 
  recipe(species ~ .) %>%
  step_impute_mode(all_nominal()) %>% 
  step_impute_mean(all_numeric()) %>% 
  step_dummy(all_nominal(), -all_outcomes()) %>% 
  step_integer(all_nominal()) %>% 
  step_normalize(all_predictors(), -all_outcomes()) %>% 
  prep() %>% juice()

penguin_data %>% dim()
```

]

]
.panel[.panel-name[입력 features]

.left-column[

]
.right-column[

```{r}
penguin_data %>% head()
```

]

]

.panel[.panel-name[Dataset 클래스]
.left-column[

]
.right-column[
```{r}
penguin_dataset <- dataset(
  name = "penguin_data",
  initialize = function() {
      self$data <- torch_tensor(as.matrix(select(penguin_data, species, everything())))
  },
  .getitem = function(index) {
      x <- self$data[index, 2:9]
      y <- self$data[index, 1]
      
      list(x, y)
  },
  .length = function() {
        self$data$size()[[1]]
  }
)
```
]
]

]

---
# 펭귄 데이터셋

.panelset[
.panel[.panel-name[데이터 객체생성]
.left-column[
데이터 객체생성
]
.right-column[
```{r}
torch_penguin_data <- penguin_dataset()
torch_penguin_data
```
]
]
.panel[.panel-name[데이터 샘플]

.left-column[

]
.right-column[
```{r}
torch_penguin_data$.getitem(1:6)
```

]

]

.panel[.panel-name[데이터로더]
.left-column[

]
.right-column[
```{r}
penguin_dl <- dataloader(torch_penguin_data, batch_size = 8)
penguin_dl$.length()
```

]
]

.panel[.panel-name[batch 구조이해]
.left-column[

]
.right-column[
```{r}
b <- penguin_dl$.iter()$.next()
length(b)
b[[1]]
```

]
]

]

---
# 학습하기

.panelset[
.panel[.panel-name[학습준비]
.left-column[

]
.right-column[
```{r}
device <- if (cuda_is_available()) torch_device("cuda:0") else "cpu"
my_net <- my_net$to(device = device)

criterion <- nn_cross_entropy_loss()
optimizer <- optim_sgd(my_net$parameters, 
                       lr = 0.1, momentum = 0.9)
num_epochs <- 5

my_net$train()

```

]
]
.panel[.panel-name[학습루프]
.left-column[

]
.right-column[
```{r train-net, results = "hold"}
for (epoch in 1:num_epochs) {
    train_losses <- c()  
    
    coro::loop(for (b in penguin_dl) {
        optimizer$zero_grad()
        output <- my_net(b[[1]]$to(device = device))
        loss <- criterion(output, b[[2]]$to(device = device, dtype  = torch_long()))
        loss$backward()
        optimizer$step()
        loss <- loss$item()
        train_losses <- c(train_losses, loss)
    })
    cat(sprintf("\nLoss at epoch %d: training: %3f\n", epoch, mean(train_losses)))
}
```
]
]
.panel[.panel-name[학습결과]
.left-column[

]
.right-column[
```{r ref.label='train-net', echo=FALSE}
```

]
]
]


