---
title: "WalMart assignment"
output:
  html_document:
    number_sections: true
    fig_caption: true
    toc: true
    fig_width: 5
    fig_height: 4
    theme: cosmo
    highlight: tango
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center")
```

# Preparations (준비작업) {.tabset .tabset-fade}

## Libraries


```{r load_lib, message=FALSE, warning=FALSE, results='hide'}
library(tidymodels)
library(tidyverse)
library(magrittr)
library(skimr)
library(knitr)
theme_set(theme_bw())
```

## Data load

```{r}
file_path <- "../input/walmart-recruiting-store-sales-forecasting/"
files <- list.files(file_path)
files
```


```{r, message=FALSE}
train <- read_csv(file.path(file_path, "train.csv")) %>% 
  janitor::clean_names()
test <- read_csv(file.path(file_path, "test.csv")) %>% 
  janitor::clean_names()
features <- read_csv(file.path(file_path, "features.csv")) %>% 
  janitor::clean_names()
stores <- read_csv(file.path(file_path, "stores.csv")) %>% 
  janitor::clean_names()
```


# 데이터 기본정보 확인{.tabset .tabset-fade}

## Basic info.

이 대회는 기본적으로 간단한 대회이다. 첫번째 스터디용 대회로 선택을 한 이유이기도 하다. 주 데이터는 42만개의 train 샘플과 11만개의 test 샘플로 구성이 되어있다.

```{r}
dim(train)
dim(test)
```

변수명을 살펴보면, 월마트 가맹점을 뜻하는 `store` 변수와 매장안의 부서들을 나타내는 `dept`, 날짜 정보를 가지고 있는 `date`와 `is_holiday`, 마지막으로 우리의 target 변수인 `weekly_sales`가 있는 것을 확인 할 수 있다.

```{r}
names(train)
names(test)
```

## train data

```{r}
skim(train)
```

## test data

```{r}
skim(test)
```

## store data

`store` 데이터는 상대적으로 간단하다. 각 점포에 대한 사이즈와 타입변수가 담겨져 있다. 타입변수는 월마트에서 운영하는 supercenter와 같이 매장의 성격을 나타내는 변수이다.

```{r}
dim(stores)
head(stores)
```


```{r}
skim(stores)
```

## feature data

`feature` 데이터는 조금 복잡한데, 각 점포별로 각 주마다의 정보가 담겨있는 것을 알 수 있다.

```{r}
dim(features)
length(unique(features$Store)) * length(unique(features$Date))

head(features)
```

일단 `NA`의 존재가 많음. `skim()` 함수의 complete 정보를 통하여 알아 볼 수 있다. 또한, 대회 데이터에 대한 설명을 보면 `mark_down1-5` 변수의 경우 월마트에서 진행하고 있는 Promotion을 의미한다. 하지만 이 변수의 경우 2011년 11월 이후에 날짜에 대하여만 접근 가능하고, 그 이전의 경우에는 `NA`로 채워져있다. 이러한 `NA`를 어떻게 사용할 것인가가 이 대회의 핵심일 것 같다.

```{r}
skim(features)
```


# 탐색적 데이터 분석 및 시각화 {.tabset .tabset-fade}

## `weekly_sales`

먼저 우리의 예측 목표인 주간 세일변수 `weekly_sales`를 시각화 해보도록 하자.

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
  ggplot(aes(x = weekly_sales)) +
  geom_histogram()
```
매출액의 분포라서 오른쪽으로 엄청 치우쳐있는 것을 알 수 있다. 이런 경우 보통 `log` 함수를 취해줘서 분포의 치우침을 잡아준다. 이렇게 분포 치우침을 잡아주는 이유는 회귀분석 같은 전통적인 기법의 경우 데이터에 섞여있는 잡음의 분포를 정규분포같이 대칭인 분포로 가정하는 경우가 많기 때문이다. 

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
    ggplot(aes(x = sign(weekly_sales) * log(abs(weekly_sales) + 2))) +
    geom_histogram() +
    labs(title = "Transformed distribution of weekly sales 1",
         x = "weekly_sales")
```

`log`를 취해주었을 경우 다음과 같이 치우침이 많이 잡히는 것을 알 수 있다. 하지만 위의 분포 역시 왼쪽으로 치우쳐 있는 것을 알 수 있다. 분포를 조금 더 종모양으로 만들어주기 위하여 제곱근을 이용했다. 아래를 보면 분포가 종모양처럼 예뻐진 것을 알 수 있다.

```{r message=FALSE, class.source = 'fold-hide'}
train %>% 
    ggplot(aes(x = sign(weekly_sales) * (abs(weekly_sales))^(1/5))) +
    geom_histogram() +
    labs(title = "Transformed distribution of weekly sales 2",
         x = "weekly_sales")
```

## NA analysis

R에는 결측치 분석을 아주 용이하게 해주는 패키지가 하나 존재하는데, 바로 `naniar`라는 패키지 이다.

```{r message=FALSE, warning=FALSE, class.source = 'fold-hide'}
library(naniar)
features %>% 
  select_if(~sum(is.na(.)) > 0) %>% # 결측치 있는 칼럼 선택
  gg_miss_var()
```
`gg_miss_var()`를 통하여 현재 mark_down1-5 변수, 그리고, unemploment와 cpi가 결측치가 존재하는 것을 확인하였다.

```{r message=FALSE, class.source = 'fold-hide'}
features %>% 
  select_if(~sum(is.na(.)) > 0) %>%
  gg_miss_upset()
```

`gg_miss_upset()` 함수의 경우 결측치가 동시에 발생하는 변수들을 보여주는데, mark_down1-5까지가 동시에 없는 결측치의 경우 (첫번째 기둥) 2011년 11월 이전의 자료를 나타내는 것을 알 수 있다.

# 전처리 레시피(`recipe`) 만들기

tidymodels의 전처리 패키지지 recipe을 사용하여 전처리를 하도록하자.

## `all_data` 합치기

먼저 `store` 와 `features` 데이터에 있는 정보를 `train`과 `test` 데이터에 옮겨오자. 일단 결측치가 없는 변수들만 가져오고, 추후에 결측치가 있는 변수인 cpi와 unemployment, mark_down 변수들을 가져오자.

```{r}
# train weekly_sales 변경
train %<>%
  mutate(weekly_sales = sign(weekly_sales) * (abs(weekly_sales))^(1/5))

all_data <- bind_rows(train, test)
all_data <- left_join(all_data, stores, by = c("store"= "store"))
all_data <- features %>% 
    select(-c(starts_with("mark"), is_holiday)) %>% 
    left_join(all_data, y = ., by = c("store"= "store",
                                      "date" = "date"))

names(all_data)
dim(all_data)
```

## `NA` cpi와 unemployment 변수

`unemployment`과 `cpi`의 `NA` 값들의 경우, 결측치 값이 2013년 5, 6, 7월에 집중 되어 있는 것을 확인 할 수 있다.

```{r}
all_data %>% 
    mutate(year = lubridate::year(date)) %>% 
    mutate(month = lubridate::month(date)) %>% 
    group_by(year, month) %>% 
    summarise(count_na_cpi = sum(is.na(cpi)),
              count_na_unemp = sum(is.na(unemployment))) %>% 
    filter(count_na_cpi > 0 | count_na_unemp > 0)
```

## `cpi`와 `unemployment` 변수 `NA` 결측치 채우기

`cpi`와 `unemployment`를 종속변수로 설정 후 년도, 월, 점포, 부서 변수를 이용해서 회귀분석을 이용하여 채우도록 한다.

```{r}
impute_var <- function(var, all_data, var_name){
  var_train <- all_data %>% 
    select({{var}}, date, store, dept) %>% 
    filter(!is.na({{var}}))
  var_test <- all_data %>% 
    select({{var}}, date, store, dept) %>% 
    filter(is.na({{var}}))
  
  var_rec <- recipe(as.formula(paste0(var_name, "~ .")), var_train) %>% 
      step_mutate(store = as_factor(store),
                  dept = as_factor(dept),
                  year = lubridate::year(date),
                  month = lubridate::month(date)) %>%
      step_dummy(store, dept) %>% 
      prep(training = var_train)
  var_train2 <- juice(var_rec)
  var_test2 <- bake(var_rec, var_test)
  
  lm_model <- 
      linear_reg() %>%
      set_engine("lm")
  
  lm_fit <- 
      lm_model %>% 
      fit(as.formula(paste0(var_name, "~ .")), data = var_train2)
  
  var_impute <- predict(lm_fit, var_test2)
  var_impute$.pred
}
memory.limit(size = 50000)
result_cpi <- impute_var(cpi, all_data, "cpi") # 메모리 한계 오류
#memory.size(max = TRUE)  #OS에서 얻은 최대 메모리 크기 = OS로부터 R이 사용 가능한 메모리
#memory.size(max = FALSE)   # 현재 사용중인 메모리 크기
#memory.limit(size = NA)    # 컴퓨터의 최대 메모리 한계치 
 # 컴퓨터의 최대 메모리 한계치 약 49GB로 높이기

result_ump <- impute_var(unemployment, all_data, "unemployment")
all_data$cpi[is.na(all_data$cpi)] <- result_cpi
all_data$unemployment[is.na(all_data$unemployment)] <- result_ump
all_data %>% tail %>% kable()
```

```{r}
all_data %>% 
    summarise_all(~sum(is.na(.)))
```


```{r}
  all_data %>% head()
  info_cpi <- all_data %>% 
   group_by(store, dept) %>% 
   summarise(mean_cpi = mean(cpi, na.rm = TRUE),
             mean_unemp = mean(unemployment, na.rm = TRUE))
 info_cpi
 all_data %>% 
   filter(is.na(cpi) | is.na(unemployment))
```


## 공휴일 데이터 코딩

미국의 휴일 정보를 가지고있는 `step_holiday` 함수를 이용해서 미국 공휴일을 모두 빼오도록 한다. 다음은 미국 공휴일 목록이다.

```{r}
timeDate::listHolidays("US")
```

```{r}
library(lubridate)

datedb <- data.frame(date = ymd("2010-1-1") + days(0:(365*4))) %>% 
    filter(date > "2010-01-29" & date < "2013-07-27") %>% 
    mutate(index = 0:(length(date)-1))
datedb$date %>% range()
all_data$date %>% range()

holiday_rec <- recipe(~ date + index, datedb) %>% 
    step_holiday(date,
                 holidays = timeDate::listHolidays("US")) %>% 
    step_mutate(index_mod = index %/% 7) %>% 
    prep(training = datedb) %>% 
    juice()

holiday_rec %<>%
    select(-date) %>% 
    select(starts_with("date"), index_mod) %>% 
    group_by(index_mod) %>% 
    summarise_all(sum) %>% 
    mutate(date = all_data$date %>% unique()) %>% 
    select(date, dplyr::everything())

all_data <- holiday_rec %>% 
    select(-index_mod) %>% 
    left_join(all_data, y = ., by = c("date" = "date"))
all_data %>% head() %>%
  kable()
  
# custom weights
# weight <- c(1, 5)[as_factor(all_data$is_holiday)]
```

## 전처리 과정 기록하기

tidymodel의 편리한 장점 중 하나는 다양한 전처리 함수를 제공해서 실제로 전처리 코딩을 하지 않아도 되도록 자동화 시켜놓은 것이다. 전처리를 하고자 하는 방법을 recipe에 적어주면, 나중에 한번에 전처리를 시켜준다.

다음의 recipe에는 `date` 변수에서 날짜 정보를 빼오고, `temperature, fuel_price, cpi, unemployment` 변수들을 10차항까지 코딩해서 넣어주는 전처리 과정이 들어있다.

```{r}
walmart_recipe <- all_data %>% 
    recipe(weekly_sales ~ .) %>%
    step_date(date) %>%
    step_rm(date, date_dow) %>%
    step_mutate(store = as_factor(store),
                dept = as_factor(dept)) %>%     
    step_poly(temperature, fuel_price, cpi, unemployment,
              degree = 10) %>%
    step_dummy(all_nominal()) %>% 
    step_normalize(all_numeric(), -all_outcomes()) %>%
    prep(training = all_data)

print(walmart_recipe)
```

## 전처리 데이터 짜내기 (`juice`)

저장된 `recipe`의 전처리를 한 데이터를 `juice` 함수로 짜내보자.

```{r}
all_data2 <- juice(walmart_recipe)
all_data2 %>% dim()
all_data2 %>% head() %>% 
  kable()
```

# 모델 학습하기

## 데이터 나누기

```{r}
train_index <- seq_len(nrow(train))
train2 <- all_data2[train_index,]
test2 <- all_data2[-train_index,]

```

```{r}

  ridge_model <- 
    linear_reg(penalty = 0.01, # tuned penalty
               mixture = 0) %>% # lasso: 1, ridge: 0
    set_engine("glmnet")

  ridge_fit <- 
    ridge_model %>% 
    fit(weekly_sales ~ ., data = train2)
  
  lasso_model <- 
    linear_reg(penalty = 0.01, # tuned penalty
               mixture = 1) %>% # lasso: 1, ridge: 0
    set_engine("glmnet")

  lasso_fit <- 
    lasso_model %>% 
    fit(weekly_sales ~ ., data = train2)


```


```{r warning=FALSE}
result_ridge <- predict(ridge_fit, test2)
result_ridge %>% head()
result_ridge %<>%
  mutate(.pred = sign(.pred) * (abs(.pred)^5))
```

```{r warning=FALSE}
result_lasso <- predict(lasso_fit, test2)
result_lasso %>% head()
result_lasso %<>%
  mutate(.pred = sign(.pred) * (abs(.pred)^5))
```

```{r}
submission <- read_csv(file.path(file_path, "sampleSubmission.csv"))
submission$Weekly_Sales <- result_ridge$.pred
write.csv(submission, row.names = FALSE,
          "ridge_regression.csv")
submission %>% head()
```

```{r}
submission <- read_csv(file.path(file_path, "sampleSubmission.csv"))
submission$Weekly_Sales <- result_lasso$.pred
write.csv(submission, row.names = FALSE,
          "lasso_regression.csv")
submission %>% head()
```